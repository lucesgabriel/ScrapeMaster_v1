Universal Web Scraper 

Este proyecto es una aplicaci贸n universal de scraping web que permite extraer datos de sitios web y convertirlos en un formato estructurado. Utiliza diversas APIs de inteligencia artificial para mejorar la detecci贸n de paginaci贸n y generar datos estructurados en formato JSON o CSV.

Estructura del Proyecto
markdown
Copy code
 ./  
     assets.py  
     pagination_detector.py  
     requirements.txt  
     scraper.py  
     streamlit_app.py  
Descripci贸n de Archivos:
assets.py: Contiene constantes y variables de configuraci贸n que se utilizan en diferentes partes de la aplicaci贸n. Ejemplos incluyen configuraciones de agentes de usuario, opciones de modo sin cabeza y precios de los modelos de IA.

pagination_detector.py: M贸dulo para detectar elementos de paginaci贸n en el contenido de sitios web. Utiliza modelos de IA para extraer informaci贸n y calcular los precios de las operaciones basadas en el n煤mero de tokens utilizados.

scraper.py: M贸dulo principal que maneja la extracci贸n de datos de los sitios web, incluyendo la configuraci贸n de Selenium, limpieza de HTML y conversi贸n a formato markdown. Tambi茅n permite el procesamiento de datos con modelos de IA como OpenAI, Google Gemini y Groq.

streamlit_app.py: Aplicaci贸n principal de Streamlit que proporciona una interfaz gr谩fica para el scraping. Los usuarios pueden introducir URLs, definir campos a extraer y configurar la paginaci贸n. Los resultados se pueden descargar en formato JSON o CSV.

requirements.txt: Lista de dependencias necesarias para ejecutar el proyecto.

Requisitos del Sistema
Para ejecutar este proyecto, aseg煤rate de tener los siguientes componentes instalados:

Python 3.8+
Google Chrome (para Selenium)
ChromeDriver (configurado en el PATH)
Instalaci贸n de dependencias: Utiliza el siguiente comando para instalar todas las dependencias del proyecto.
bash
Copy code
pip install -r requirements.txt
C贸mo Ejecutar la Aplicaci贸n
Configura el entorno:

Crea un archivo .env en el directorio ra铆z con tus claves API para OpenAI, Google y Groq.
bash
Copy code
OPENAI_API_KEY=tu_clave_api_openai
GOOGLE_API_KEY=tu_clave_api_google
GROQ_API_KEY=tu_clave_api_groq
Inicia la aplicaci贸n:

Ejecuta el siguiente comando para iniciar la aplicaci贸n de Streamlit:
bash
Copy code
streamlit run streamlit_app.py
Uso de la aplicaci贸n:

Ingresa una o m谩s URLs en la barra lateral.
Define los campos que deseas extraer como etiquetas.
Activa la detecci贸n de paginaci贸n si es necesario.
Haz clic en "Scrape" para iniciar el scraping.
Descarga los resultados:

Descarga los datos extra铆dos en formato JSON o CSV desde la interfaz de usuario.
Modelos Soportados
Este proyecto soporta varios modelos de IA para realizar las tareas de extracci贸n y detecci贸n de paginaci贸n:

OpenAI: Modelos como gpt-4o-mini y gpt-4o-2024-08-06.
Google Gemini: Modelo gemini-1.5-flash.
Groq Llama: Modelos Llama3.1 8B y Groq Llama3.1 70b.
Cada modelo tiene costos asociados seg煤n los tokens utilizados, que se calculan autom谩ticamente y se muestran en la aplicaci贸n.

Funcionalidades
Extracci贸n de datos: Extrae informaci贸n estructurada de sitios web usando IA.
Detecci贸n de paginaci贸n: Detecta elementos de paginaci贸n y sigue enlaces a otras p谩ginas.
Conversi贸n de HTML a Markdown: Limpia el HTML y lo convierte a formato Markdown para una mejor legibilidad.
Compatibilidad con m煤ltiples modelos de IA: Selecci贸n de modelos de IA para la extracci贸n de datos.
Licencia
Este proyecto est谩 bajo la MIT License.

Contacto
Si encuentras alg煤n error o tienes sugerencias, no dudes en crear un issue en GitHub.
